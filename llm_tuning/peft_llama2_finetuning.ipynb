{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - LLaMA2 (PEFT Finetuning)\n",
    "\n",
    "Adapted from: [Llama 2 PEFT Finetuning with Text Data on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_finetuning.ipynb)\n",
    "\n",
    "Modified by: [Wan Qi Ang](https://github.com/angwanqi) for 2024 EDB x Google Cloud - Cloud AI Take Off Program\n",
    "\n",
    "Last updated: 8 Nov 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates downloading [LLaMA2 models](https://huggingface.co/meta-llama), finetuning with parameter efficient finetuning libraries ([PEFT](https://github.com/huggingface/peft)), and deploying the finetuned model on Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Download prebuilt LLaMA2 models.\n",
    "- Finetune and deploy LLaMA2 models with Vertex AI SDK.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Run the cell below if this is your first time running the notebook. Else, feel free to skip the cell below as the libraries would have already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>NOTE:</b> Only restart the current runtime if you installed libraries. If you did not install new libraries, you do not need to restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent Resource ID\n",
    "PERSISTENT_RESOURCE_ID = \"ai-takeoff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dynamically retrieve Persistent Resource location\n",
    "PERSISTENT_RESOURCE_REGION = \"\"\n",
    "check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "for region in check_regions:\n",
    "    shell_output = !gcloud ai persistent-resources list --project=$PROJECT_ID --region=$region\n",
    "    if \"Listed 0 items.\" not in shell_output:\n",
    "        print(f\"Persistent Resource found in {region}\")\n",
    "        PERSISTENT_RESOURCE_REGION = region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your project ID and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set the default region for launching jobs.\n",
    "REGION = PERSISTENT_RESOURCE_REGION\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set (or create) the Google Cloud Storage bucket\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>INPUT REQUIRED:</b> Replace <YOUR_NAME> with your name so that you'll be able to identify your Google Cloud Storage bucket later on. <b>Example:</b> BUCKET_PREFIX = \"john\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the bucket prefix with your name\n",
    "PREFIX = \"<YOUR_NAME>\"\n",
    "\n",
    "# Concatenate to get the full bucket name\n",
    "BUCKET_NAME = PREFIX + \"-llama2-text-peft\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama2\")\n",
    "\n",
    "\n",
    "print(f\"ROOT_BUCKET_URI:\", BUCKET_URI)\n",
    "print(f\"STAGING_BUCKET_URI:\", STAGING_BUCKET)\n",
    "print(f\"EXPERIMENT_BUCKET_URI:\", EXPERIMENT_BUCKET)\n",
    "print(f\"MODEL_BUCKET_URI:\", MODEL_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Cloud Storage Bucket\n",
    "!gcloud storage buckets create $BUCKET_URI --location=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the Compute Engine Service Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the default Compute Engine Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Cloud Storage admin IAM role to the Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Llama 2\n",
    "- The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
    "- Accept the model agreement to access the models:\n",
    "- Open the [LLaMA2 model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "- Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "- A Cloud Storage bucket (starting with `gs://`) containing LLaMA 2 pretrained and finetuned models will be shared under the “Documentation” section and its “Get started” subsection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Llama 2 Model Artifacts to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Llama 2 model artifaces to your bucket\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"gs://vertex-model-garden-public-us-central1/llama2\"\n",
    "\n",
    "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/* $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zI30m3bqDtCj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the training dataset\n",
    "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ax6GlzzMk-sc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"\n",
    "\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and have a look at what's in it\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "\n",
    "# Print the first 2 rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the first row of data\n",
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finetune with PEFT\n",
    "This section demonstrates how to finetune the LLaMA 2 models with PEFT LoRA. By default, the model will be finetuned for 500 steps on a batch size of 1 to save GPU resources.\n",
    "- Finetuning `llama2-7b` models is expected to take around 30 minutes.\n",
    "- To customize finetuning settings and parameters, click \"Show code\" to see more details.\n",
    "\n",
    "Here are the full list of models that you can tune:\\\n",
    "[\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the base model ID that you would like to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base model id.\n",
    "base_model_id = \"llama2-7b-hf\" \n",
    "model_id = os.path.join(MODEL_BUCKET, base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the accelerator that you would like to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the accelerator type.\n",
    "accelerator_type = \"NVIDIA_A100_80GB\"\n",
    "\n",
    "machine_type = None\n",
    "if \"7b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-16\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1 \n",
    "elif \"13b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-32\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1\n",
    "elif \"70b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-4g\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        accelerator_count = 8\n",
    "\n",
    "if machine_type is None:\n",
    "    raise ValueError(\n",
    "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to set an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` in worker_pool_specs.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "e1289e21a9d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = get_job_name_with_datetime(PREFIX + \"-llama2-train\")\n",
    "output_dir = os.path.join(EXPERIMENT_BUCKET, job_name)\n",
    "merge_job_name = get_job_name_with_datetime(PREFIX + \"-llama2-merge\")\n",
    "merged_model_output_dir = os.path.join(EXPERIMENT_BUCKET, merge_job_name)\n",
    "finetune_precision_mode = \"float16\"\n",
    "\n",
    "# The pre-built training docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:latest\"\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Runs 500 training steps.\n",
    "max_steps = 500\n",
    "per_device_train_batch_size = 1\n",
    "# LoRA parameters.\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "\n",
    "flags = {\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"precision_mode\": finetune_precision_mode,\n",
    "    \"task\": \"instruct-lora\",\n",
    "    \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"instruct_column_in_dataset\": instruct_column_in_dataset,\n",
    "    \"template\": template,\n",
    "    \"pretrained_model_id\": model_id,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"merge_base_and_lora_output_dir\": merged_model_output_dir,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"lora_rank\": lora_rank,\n",
    "    \"lora_alpha\": lora_alpha,\n",
    "    \"lora_dropout\": lora_dropout,\n",
    "}\n",
    "\n",
    "train_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=[\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": replica_count,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAIN_DOCKER_URI,\n",
    "                \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    persistent_resource_id=PERSISTENT_RESOURCE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ It will take ~45mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_job.run(service_account=SERVICE_ACCOUNT)\n",
    "\n",
    "print(\"The finetuned models of different trials can be found at: \", output_dir)\n",
    "print(\n",
    "    \"The finetuned model merged with the base model can be found at: \",\n",
    "    merged_model_output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying your finetuned model\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 4096,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.8\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--max-num-batched-tokens=4096\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\"MODEL_ID\": model_id, \"DEPLOY_SOURCE\": \"notebook\"}\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        artifact_uri=model_id,\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    print(\"To load this existing endpoint from a different session:\")\n",
    "    print(\"from google.cloud import aiplatform\")\n",
    "    print(\n",
    "        f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "h0hGj09CuRFQ"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving and training docker images.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240326_0916_RC00\"\n",
    "\n",
    "print(\"Deploying models in: \", merged_model_output_dir)\n",
    "\n",
    "# The max_model_len must not exceed the model's context length.\n",
    "# A larger max_model_len will require more GPU memory.\n",
    "max_model_len = 2048\n",
    "machine_type = None\n",
    "accelerator_type = \"NVIDIA_A100_80GB\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "\n",
    "if \"7b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-8\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1\n",
    "elif \"13b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-16\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "elif \"70b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-4g\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        accelerator_count = 8\n",
    "\n",
    "if machine_type is None:\n",
    "    raise ValueError(\n",
    "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model_vllm function.\"\n",
    "    )\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"llama-vllm-serve\"),\n",
    "    model_id=merged_model_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing the endpoint with a prediction\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:\n",
    "\n",
    "<b>Human:</b> How would the Future of AI in 10 Years look?\\\n",
    "<b>Assistant:</b> Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years:\n",
    "- <b>Continued advancements in deep learning:</b> Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks.\n",
    "- <b>Increased use of AI in healthcare:</b> AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes.\n",
    "- <b>Greater automation in the workplace:</b> Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills.\n",
    "- <b>More natural and intuitive interactions with technology:</b> As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants.\n",
    "- <b>Increased focus on ethical considerations:</b> As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. \n",
    "\n",
    "Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# # Retrieve the Vertex AI Prediction Endpoint IDs and set it\n",
    "# endpoint = \"\"\n",
    "# check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "# for region in check_regions:\n",
    "#     all_endpoints = aiplatform.Endpoint.list(location=region)\n",
    "#     for endpoint in all_endpoints:\n",
    "#         full_endpoint = f\"projects/{PROJECT_ID}/locations/{region}/endpoints/{endpoint.name}\"\n",
    "        \n",
    "#         if \"llama-vllm-serve\" in endpoint.display_name:\n",
    "#             endpoint = aiplatform.Endpoint(full_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vgU_qYHNuy3w"
   },
   "outputs": [],
   "source": [
    "prompt = \"How would the Future of AI in 10 Years look?\"\n",
    "max_tokens = 128\n",
    "temperature = 1.0\n",
    "top_p = 0.9\n",
    "top_k = 1\n",
    "\n",
    "# Overides max_tokens and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_tokens as 20.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": f\"### Human: {prompt}### Assistant: \",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean up resources\n",
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "if train_job._gca_resource.name:\n",
    "    # Training job is submitted.\n",
    "    train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model.\n",
    "model.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created.\n",
    "delete_bucket = False  # @param {type:\"boolean\"}\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama2_peft_finetuning.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
