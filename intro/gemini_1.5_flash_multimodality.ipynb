{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCIMTPB1WoTq"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yVV6txOmNMn"
   },
   "source": [
    "# Getting started with Vertex AI Gemini 1.5 Flash\n",
    "\n",
    "Adapted from: [Intro to Gemini 1.5 Flash on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_1_5_flash.ipynb)\n",
    "\n",
    "Author(s): [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner)\n",
    "\n",
    "Modified by: [Wan Qi Ang](https://github.com/angwanqi) for 2024 EDB x Google Cloud - Cloud AI Take Off Program\n",
    "\n",
    "Last updated: 29 October 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DnOs6rkbOy"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Gemini 1.5 Flash is a new language model from the Gemini family. This model includes the long context window of up to 1 million tokens from Gemini 1.5 Pro and is optimized for low-latency tasks. It can process text, images, audio, video, and code all together for deeper insights. Learn more about [Gemini 1.5 Flash](https://deepmind.google/technologies/gemini/flash/).\n",
    "\n",
    "With this tutorial, you learn how to use the Vertex AI Gemini API and the Vertex AI SDK to work with the Gemini 1.5 Flash model to:\n",
    "\n",
    "- analyze audio for insights.\n",
    "- understand videos (including their audio components).\n",
    "- extract information from PDF documents.\n",
    "- process images, video, audio, and text simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>NOTE:</b> Only restart the current runtime if you installed libraries. If you did not install new libraries, you do not need to restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01",
    "tags": []
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import os\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve and set the Project ID\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set the default region for launching jobs.\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)\n",
    "\n",
    "# Initialize Vertex AI with Project ID and Region\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY1nfXrqRxVX"
   },
   "source": [
    "### Load the Gemini 1.5 Flash model\n",
    "\n",
    "To learn more about all [Gemini API models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7ExWmuLBdIA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-1.5-flash-002\"\n",
    "\n",
    "model = GenerativeModel(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9OKM0-4SQf8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Vertex AI SDK basic usage\n",
    "\n",
    "Below is a simple example that demonstrates how to prompt the Gemini 1.5 Flash model using the Vertex AI SDK. Learn more about the [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini#gemini-pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a example model with system instructions\n",
    "example_model = GenerativeModel(\n",
    "    MODEL_ID,\n",
    "    system_instruction=[\n",
    "        \"You are a helpful language translator.\",\n",
    "        \"Your mission is to translate text in English to French.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Set model parameters\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=8192,\n",
    ")\n",
    "\n",
    "# Set safety settings\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with you own prompts!\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Set contents to send to the model\n",
    "contents = [prompt]\n",
    "\n",
    "# Counts tokens\n",
    "print(\"Before sending the prompt, you can check the token count...\")\n",
    "print(f\"Prompt: {contents}\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(example_model.count_tokens(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhFxrtfdSwOP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt the model to generate content\n",
    "response = example_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")\n",
    "\n",
    "# Print the model response\n",
    "prompt\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"\\nAnswer:\\n{response.text}\")\n",
    "print(f'\\nUsage metadata:\\n{response.to_dict().get(\"usage_metadata\")}')\n",
    "print(f\"\\nFinish reason:\\n{response.candidates[0].finish_reason}\")\n",
    "print(f\"\\nSafety settings:\\n{response.candidates[0].safety_ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acRxKRA-sr0j"
   },
   "source": [
    "## Audio understanding\n",
    "\n",
    "Gemini 1.5 Flash can directly process audio for long-context understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10hgCOIA4E5_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_file_path = \"cloud-samples-data/generative-ai/audio/pixel.mp3\"\n",
    "audio_file_uri = f\"gs://{audio_file_path}\"\n",
    "audio_file_url = f\"https://storage.googleapis.com/{audio_file_path}\"\n",
    "\n",
    "IPython.display.Audio(audio_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sXM19QQ4vj1"
   },
   "source": [
    "#### Example 1: Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPQ1fBk44E6L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Please provide a summary for the audio.\n",
    "  Provide chapter titles, be concise and short, no need to provide chapter summaries.\n",
    "  Do not make up any information that is not part of the audio and do not be verbose.\n",
    "\"\"\"\n",
    "\n",
    "audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
    "contents = [audio_file, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzA8vKgQATGL"
   },
   "source": [
    "#### Example 2: Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buziSRMG-42a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Can you transcribe this interview, in the format of speaker, caption.\n",
    "    Use speaker A, speaker B, etc. to identify the speakers.\n",
    "\"\"\"\n",
    "\n",
    "audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
    "contents = [audio_file, prompt]\n",
    "\n",
    "responses = model.generate_content(contents, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U36v4TmswAG"
   },
   "source": [
    "## Video with audio understanding\n",
    "\n",
    "Try out Gemini 1.5 Flash's native multimodal and long context capabilities on video interleaving with audio inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDswcPI0tSRk"
   },
   "outputs": [],
   "source": [
    "video_file_path = \"cloud-samples-data/generative-ai/video/pixel8.mp4\"\n",
    "video_file_uri = f\"gs://{video_file_path}\"\n",
    "video_file_url = f\"https://storage.googleapis.com/{video_file_path}\"\n",
    "\n",
    "IPython.display.Video(video_file_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9isZfjzCYxw"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Provide a description of the video.\n",
    "  The description should also contain anything important which people say in the video.\n",
    "\"\"\"\n",
    "\n",
    "video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
    "contents = [video_file, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcBZZ-bJe2yS"
   },
   "source": [
    "Gemini 1.5 Flash model is able to process the video with audio, retrieve and extract textual and audio information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dTcKyoutS7U"
   },
   "source": [
    "## PDF document analysis\n",
    "\n",
    "You can use Gemini 1.5 Flash to process PDF documents, and analyze content, retain information, and provide answers to queries regarding the documents.\n",
    "\n",
    "The PDF document example used here is the Gemini 1.5 paper (https://arxiv.org/pdf/2403.05530.pdf).\n",
    "\n",
    "![image.png](https://storage.googleapis.com/cloud-samples-data/generative-ai/image/gemini1.5-paper-2403.05530.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgKDIZUstYwV"
   },
   "outputs": [],
   "source": [
    "pdf_file_uri = \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  You are a very professional document summarization specialist.\n",
    "  Please summarize the given document.\n",
    "\"\"\"\n",
    "\n",
    "pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3vu8ogWs7iZ"
   },
   "source": [
    "## All modalities (images, video, audio, text) at once\n",
    "\n",
    "Gemini 1.5 Flash is natively multimodal and supports interleaving of data from different modalities, it can support a mix of audio, visual, text, and\n",
    "code inputs in the same input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp216wxgiKg4"
   },
   "outputs": [],
   "source": [
    "video_file_path = \"cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
    "video_file_uri = f\"gs://{video_file_path}\"\n",
    "video_file_url = f\"https://storage.googleapis.com/{video_file_path}\"\n",
    "\n",
    "IPython.display.Video(video_file_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS7KSwvbjhFh"
   },
   "outputs": [],
   "source": [
    "image_file_path = \"cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\"\n",
    "image_file_uri = f\"gs://{image_file_path}\"\n",
    "image_file_url = f\"https://storage.googleapis.com/{image_file_path}\"\n",
    "\n",
    "IPython.display.Image(image_file_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRdzwDi9iLGn"
   },
   "outputs": [],
   "source": [
    "video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
    "image_file = Part.from_uri(image_file_uri, mime_type=\"image/png\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Look through each frame in the video carefully and answer the questions.\n",
    "  Only base your answers strictly on what information is available in the video attached.\n",
    "  Do not make up any information that is not part of the video and do not be too\n",
    "  verbose, be to the point.\n",
    "\n",
    "  Questions:\n",
    "  - When is the moment in the image happening in the video? Provide a timestamp.\n",
    "  - What is the context of the moment and what does the narrator say about it?\n",
    "\"\"\"\n",
    "\n",
    "contents = [video_file, image_file, prompt]\n",
    "\n",
    "response = model.generate_content(contents)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3iovYxOwOT7"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've learned how to use Gemini 1.5 Flash with the Vertex AI SDK to:\n",
    "\n",
    "- analyze audio for insights.\n",
    "- understand videos (including their audio components).\n",
    "- extract information from PDF documents.\n",
    "- process images, video, audio, and text simultaneously."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_gemini_1_5_flash.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
